---
title: Monitoring & Alerting
description: Comprehensive monitoring setup with Prometheus metrics, Grafana dashboards, and multi-channel alerting
published: true
featured: true
---

# Monitoring & Alerting

KoreShield provides comprehensive monitoring and alerting capabilities with Prometheus metrics, pre-built Grafana dashboards, and multi-channel notifications.

## Overview

Monitor and alert on:

- **Detection Performance** - Accuracy, latency, throughput
- **Threat Landscape** - Attack types, severity, trends
- **System Health** - API availability, resource usage
- **Business Metrics** - Requests, blocks, costs
- **SLA Compliance** - Uptime, response times

---

## Prometheus Metrics

### Request Metrics

Track all incoming requests:

```prometheus
# Total requests by status
koreshield_requests_total{status="allowed|blocked|warned"} counter

# Request duration
koreshield_request_duration_seconds{quantile="0.5|0.9|0.95|0.99"} histogram

# Requests in flight
koreshield_requests_in_flight gauge

# Request size
koreshield_request_size_bytes{quantile="0.5|0.9|0.95|0.99"} histogram
```

**Example Query:**
```promql
# Requests per second
rate(koreshield_requests_total[5m])

# 95th percentile latency
histogram_quantile(0.95, rate(koreshield_request_duration_seconds_bucket[5m]))

# Block rate percentage
(rate(koreshield_requests_total{status="blocked"}[5m]) / 
 rate(koreshield_requests_total[5m])) * 100
```

### Detection Metrics

Monitor detection engine performance:

```prometheus
# Attacks detected by type
koreshield_attacks_detected_total{type="prompt_injection|jailbreak|..."} counter

# Detection confidence scores
koreshield_detection_confidence{quantile="0.5|0.9|0.95|0.99"} histogram

# False positives
koreshield_false_positives_total counter

# False negatives
koreshield_false_negatives_total counter

# Detection engine latency
koreshield_detection_latency_seconds{component="pattern_matcher|ml_classifier|..."} histogram
```

**Example Query:**
```promql
# Most common attack types (last hour)
topk(5, rate(koreshield_attacks_detected_total[1h]))

# False positive rate
rate(koreshield_false_positives_total[24h]) / 
rate(koreshield_requests_total[24h])

# Detection accuracy (derived)
1 - ((false_positives + false_negatives) / total_requests)
```

### System Health Metrics

Track system performance:

```prometheus
# API availability
koreshield_api_up{endpoint="/health|/status|..."} gauge

# Provider health
koreshield_provider_health{provider="openai|anthropic|..."} gauge

# Cache hit rate
koreshield_cache_hits_total counter
koreshield_cache_misses_total counter

# Database connections
koreshield_db_connections{state="active|idle"} gauge

# Redis connections
koreshield_redis_connections gauge

# Memory usage
koreshield_memory_bytes gauge

# CPU usage
koreshield_cpu_usage_percent gauge
```

**Example Query:**
```promql
# Cache hit rate
rate(koreshield_cache_hits_total[5m]) / 
(rate(koreshield_cache_hits_total[5m]) + rate(koreshield_cache_misses_total[5m]))

# Provider availability (last 5 minutes)
avg_over_time(koreshield_provider_health[5m])
```

### Business Metrics

Track usage and costs:

```prometheus
# Tenant requests
koreshield_tenant_requests_total{tenant_id="..."} counter

# Tenant costs
koreshield_tenant_cost_dollars{tenant_id="..."} gauge

# API key usage
koreshield_api_key_requests_total{api_key_id="..."} counter

# Quota usage
koreshield_quota_usage{tenant_id="...", quota_type="requests|storage"} gauge
```

---

## Grafana Dashboards

### Pre-Built Dashboards

KoreShield includes 5 production-ready dashboards:

#### Executive Overview Dashboard
```json
{
  "dashboard": "koreshield-executive",
  "panels": [
    "Total Requests (24h)",
    "Threats Blocked (24h)",
    "Block Rate Trend",
    "Top Attack Types",
    "Cost Per Day",
    "SLA Uptime"
  ]
}
```

**Import:**
```bash
# Download from GitHub
curl -O https://raw.githubusercontent.com/koreshield/koreshield/main/monitoring/grafana/executive-dashboard.json

# Import dashboard ID: 18001
```

#### Security Operations Dashboard
```json
{
  "dashboard": "koreshield-security",
  "panels": [
    "Real-Time Attack Feed",
    "Attack Severity Distribution",
    "Top Attacked Endpoints",
    "Geographic Attack Map",
    "Attack Timeline",
    "Threat Intelligence Feed",
    "Incident Response Metrics"
  ]
}
```

**Dashboard ID:** 18002

#### Performance Dashboard
```json
{
  "dashboard": "koreshield-performance",
  "panels": [
    "Request Latency (p50, p95, p99)",
    "Detection Engine Latency",
    "Throughput (req/s)",
    "Error Rate",
    "Cache Hit Rate",
    "Resource Usage (CPU, Memory)",
    "Database Performance"
  ]
}
```

**Dashboard ID:** 18003

#### Tenant Analytics Dashboard
```json
{
  "dashboard": "koreshield-tenant",
  "panels": [
    "Requests by Tenant",
    "Cost by Tenant",
    "Quota Usage",
    "Top Tenants by Volume",
    "Tenant Health Score",
    "Per-Tenant Block Rate"
  ]
}
```

**Dashboard ID:** 18004

#### SLA Compliance Dashboard
```json
{
  "dashboard": "koreshield-sla",
  "panels": [
    "Uptime (99.9% target)",
    "Latency SLA (p99 < 100ms)",
    "Error Budget Remaining",
    "Incidents This Month",
    "MTTR (Mean Time To Resolve)",
    "SLA Violations"
  ]
}
```

**Dashboard ID:** 18005

### Custom Dashboards

Create custom dashboards with PromQL:

```json
{
  "dashboard": "custom-dashboard",
  "panels": [
    {
      "title": "Block Rate by Hour",
      "query": "rate(koreshield_requests_total{status=\"blocked\"}[1h])",
      "type": "graph"
    },
    {
      "title": "Top Blocked Tenants",
      "query": "topk(10, sum by (tenant_id) (rate(koreshield_requests_total{status=\"blocked\"}[24h])))",
      "type": "table"
    },
    {
      "title": "Detection Confidence Distribution",
      "query": "histogram_quantile(0.95, rate(koreshield_detection_confidence_bucket[5m]))",
      "type": "heatmap"
    }
  ]
}
```

---

## Alerting

### Alert Channels

KoreShield supports multiple alert channels:

#### Email Alerts
```yaml
alerting:
  channels:
    - type: email
      name: "Security Team Email"
      config:
        recipients:
          - security@company.com
          - oncall@company.com
        smtp_host: smtp.gmail.com
        smtp_port: 587
        smtp_user: alerts@company.com
        smtp_password: "${SMTP_PASSWORD}"
```

#### Slack Alerts
```yaml
alerting:
  channels:
    - type: slack
      name: "Security Slack Channel"
      config:
        webhook_url: "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXX"
        channel: "#security-alerts"
        username: "KoreShield"
        icon_emoji: ":shield:"
```

#### PagerDuty Alerts
```yaml
alerting:
  channels:
    - type: pagerduty
      name: "On-Call PagerDuty"
      config:
        integration_key: "your-pagerduty-integration-key"
        severity: "critical"
```

#### Webhook Alerts
```yaml
alerting:
  channels:
    - type: webhook
      name: "Custom Webhook"
      config:
        url: "https://your-company.com/webhook/alerts"
        method: POST
        headers:
          Authorization: "Bearer ${WEBHOOK_TOKEN}"
          Content-Type: "application/json"
```

#### Microsoft Teams
```yaml
alerting:
  channels:
    - type: teams
      name: "Security Team Teams"
      config:
        webhook_url: "https://outlook.office.com/webhook/..."
```

### Alert Rules

#### High Attack Rate
```yaml
alerts:
  - name: "High Attack Rate"
    description: "More than 10% of requests are being blocked"
    query: |
      (rate(koreshield_requests_total{status="blocked"}[5m]) / 
       rate(koreshield_requests_total[5m])) * 100 > 10
    severity: warning
    duration: 5m
    channels:
      - security-team-email
      - security-slack
    annotations:
      summary: "High attack rate detected"
      description: "{{ $value | printf \"%.2f\" }}% of requests are being blocked"
```

#### Critical Threat Detected
```yaml
alerts:
  - name: "Critical Threat Detected"
    description: "High-confidence critical threat detected"
    query: |
      koreshield_attacks_detected_total{severity="critical"} > 0
    severity: critical
    duration: 1m
    channels:
      - oncall-pagerduty
      - security-slack
    annotations:
      summary: "Critical threat detected"
      description: "Attack type: {{ $labels.type }}, Confidence: {{ $value }}"
```

#### API Down
```yaml
alerts:
  - name: "API Unavailable"
    description: "KoreShield API is down"
    query: |
      koreshield_api_up == 0
    severity: critical
    duration: 1m
    channels:
      - oncall-pagerduty
      - engineering-slack
    annotations:
      summary: "KoreShield API is unavailable"
      description: "Endpoint {{ $labels.endpoint }} is down"
```

#### High Latency
```yaml
alerts:
  - name: "High Latency"
    description: "p99 latency exceeds 100ms"
    query: |
      histogram_quantile(0.99, 
        rate(koreshield_request_duration_seconds_bucket[5m])) > 0.1
    severity: warning
    duration: 10m
    channels:
      - engineering-slack
    annotations:
      summary: "High latency detected"
      description: "p99 latency: {{ $value | printf \"%.0f\" }}ms"
```

#### Provider Down
```yaml
alerts:
  - name: "Provider Unavailable"
    description: "LLM provider health check failing"
    query: |
      koreshield_provider_health < 1
    severity: warning
    duration: 5m
    channels:
      - oncall-pagerduty
    annotations:
      summary: "Provider {{ $labels.provider }} is unavailable"
```

#### Quota Exceeded
```yaml
alerts:
  - name: "Tenant Quota Exceeded"
    description: "Tenant has exceeded their quota"
    query: |
      koreshield_quota_usage > 0.9
    severity: warning
    duration: 5m
    channels:
      - billing-team-email
    annotations:
      summary: "Tenant {{ $labels.tenant_id }} quota at {{ $value | printf \"%.0f\" }}%"
```

#### High False Positive Rate
```yaml
alerts:
  - name: "High False Positive Rate"
    description: "False positive rate exceeds 2%"
    query: |
      (rate(koreshield_false_positives_total[1h]) / 
       rate(koreshield_requests_total[1h])) * 100 > 2
    severity: warning
    duration: 30m
    channels:
      - ml-team-slack
    annotations:
      summary: "False positive rate: {{ $value | printf \"%.2f\" }}%"
```

### Alert Severity Levels

```yaml
severity_levels:
  critical:
    description: "Immediate action required"
    response_time: "5 minutes"
    escalation: "Page on-call engineer"
    channels: ["pagerduty", "slack", "email"]
    
  warning:
    description: "Issue requires attention"
    response_time: "30 minutes"
    escalation: "Notify team"
    channels: ["slack", "email"]
    
  info:
    description: "Informational alert"
    response_time: "Best effort"
    escalation: "None"
    channels: ["slack"]
```

---

## Log Aggregation

### Structured Logging

KoreShield uses structured JSON logs:

```json
{
  "timestamp": "2026-02-01T10:30:00.123Z",
  "level": "info",
  "event": "request_blocked",
  "request_id": "req_abc123",
  "tenant_id": "tenant_xyz",
  "user_id": "user_456",
  "input": "Ignore previous instructions...",
  "attack_type": "prompt_injection",
  "confidence": 0.97,
  "action": "block",
  "latency_ms": 14,
  "ip_address": "192.168.1.100",
  "user_agent": "Mozilla/5.0...",
  "metadata": {
    "endpoint": "/v1/chat/completions",
    "model": "gpt-4"
  }
}
```

### Log Levels

```yaml
logging:
  level: info  # debug, info, warn, error
  format: json  # json or text
  output: stdout  # stdout, file, or both
  
  # File logging
  file:
    path: /var/log/koreshield/app.log
    max_size_mb: 100
    max_backups: 10
    max_age_days: 30
    compress: true
```

### ELK Stack Integration

Send logs to Elasticsearch:

```yaml
logging:
  elasticsearch:
    enabled: true
    hosts:
      - https://elasticsearch.company.com:9200
    index: koreshield-logs-%{+yyyy.MM.dd}
    username: elastic
    password: "${ELASTIC_PASSWORD}"
    tls:
      verify: true
```

**Kibana Queries:**
```
# All blocked requests in last hour
event:request_blocked AND timestamp:[now-1h TO now]

# High-confidence attacks
confidence:>=0.9 AND event:attack_detected

# Specific tenant activity
tenant_id:"tenant_xyz" AND level:error
```

### Splunk Integration

Forward logs to Splunk:

```yaml
logging:
  splunk:
    enabled: true
    hec_url: https://splunk.company.com:8088/services/collector
    hec_token: "${SPLUNK_HEC_TOKEN}"
    index: koreshield
    source: koreshield-api
    sourcetype: _json
```

### Datadog Integration

Send logs to Datadog:

```yaml
logging:
  datadog:
    enabled: true
    api_key: "${DATADOG_API_KEY}"
    site: datadoghq.com
    service: koreshield
    env: production
    tags:
      - team:security
      - component:detection
```

---

## Distributed Tracing

### OpenTelemetry

Enable distributed tracing:

```yaml
tracing:
  enabled: true
  provider: opentelemetry
  exporter: jaeger  # or zipkin, otlp
  
  jaeger:
    endpoint: http://jaeger.company.com:14268/api/traces
    
  sampling:
    type: probabilistic
    rate: 0.1  # Sample 10% of requests
```

### Trace Spans

KoreShield creates spans for:

1. **Request Handling** - Total request lifecycle
2. **Detection** - Each detection component
3. **Database Queries** - All DB operations
4. **Cache Operations** - Redis interactions
5. **External Calls** - LLM provider API calls

**Example Trace:**
```
Request [145ms]
  ├─ Pre-Processing [5ms]
  ├─ Pattern Matching [8ms]
  ├─ ML Classification [42ms]
  │   ├─ Tokenization [3ms]
  │   ├─ Model Inference [35ms]
  │   └─ Post-Processing [4ms]
  ├─ Semantic Analysis [25ms]
  ├─ Threat Aggregation [2ms]
  ├─ Policy Evaluation [5ms]
  └─ Response Generation [3ms]
```

---

## Health Checks

### Endpoints

```bash
# Liveness probe (is service running?)
curl http://localhost:8080/health

# Readiness probe (is service ready to serve traffic?)
curl http://localhost:8080/ready

# Detailed health check
curl http://localhost:8080/health/detailed
```

### Health Check Response

```json
{
  "status": "healthy",
  "timestamp": "2026-02-01T10:30:00Z",
  "uptime_seconds": 864000,
  "version": "2.1.0",
  "components": {
    "api": {
      "status": "healthy",
      "latency_ms": 2
    },
    "database": {
      "status": "healthy",
      "connection_pool": {
        "active": 5,
        "idle": 15,
        "max": 20
      }
    },
    "redis": {
      "status": "healthy",
      "memory_used_mb": 512,
      "connected_clients": 10
    },
    "detection_engine": {
      "status": "healthy",
      "model_version": "2.1.0",
      "last_update": "2026-01-15T00:00:00Z"
    },
    "providers": {
      "openai": {
        "status": "healthy",
        "latency_ms": 145
      },
      "anthropic": {
        "status": "healthy",
        "latency_ms": 178
      },
      "deepseek": {
        "status": "degraded",
        "latency_ms": 890,
        "error": "High latency detected"
      }
    }
  }
}
```

### Kubernetes Probes

```yaml
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: koreshield
    image: koreshield/koreshield:latest
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3
      
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
      initialDelaySeconds: 10
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 3
```

---

## Best Practices

### Layered Alerting

Create alerts at multiple levels:
- **Symptoms**: User-facing issues (high latency, errors)
- **Causes**: Root causes (database slow, cache miss rate)
- **Capacity**: Resource exhaustion (memory, connections)

### Alert Fatigue Prevention

Avoid alert fatigue:
- Group related alerts
- Use appropriate severity levels
- Set reasonable thresholds
- Implement alert suppression during maintenance

### Runbooks

Create runbooks for each alert:

```yaml
alerts:
  - name: "High Attack Rate"
    runbook_url: "https://wiki.company.com/runbooks/high-attack-rate"
    playbook:
      - "Check recent attack patterns in Grafana"
      - "Review blocked requests in last hour"
      - "Identify source IPs and block if needed"
      - "Notify security team if coordinated attack"
```

### Dashboard Organization

Organize dashboards by audience:
- **Executive**: Business metrics, costs, ROI
- **Engineering**: Performance, errors, resource usage
- **Security**: Threats, attacks, compliance
- **Operations**: Health, capacity, SLAs

### Retention Policies

Set appropriate retention:

```yaml
retention:
  metrics:
    high_resolution: 7d   # 10s resolution
    medium_resolution: 30d  # 1m resolution
    low_resolution: 365d  # 1h resolution
    
  logs:
    detailed_logs: 30d
    summary_logs: 90d
    audit_logs: 365d
    
  traces:
    all_traces: 7d
    error_traces: 30d
```

---

## Troubleshooting

### High Memory Usage

**Symptoms**: `koreshield_memory_bytes` increasing

**Diagnosis:**
```promql
# Memory growth rate
rate(koreshield_memory_bytes[1h])

# Cache size
koreshield_cache_size_bytes
```

**Solutions:**
- Reduce cache size
- Increase cache TTL
- Check for memory leaks

### Detection Latency Spike

**Symptoms**: `koreshield_detection_latency_seconds` > 100ms

**Diagnosis:**
```promql
# Latency by component
histogram_quantile(0.99, 
  rate(koreshield_detection_latency_seconds_bucket[5m]))

# Slowest component
max by (component) (koreshield_detection_latency_seconds)
```

**Solutions:**
- Check ML model performance
- Review custom rules complexity
- Optimize database queries

---

## Related Documentation

- [Attack Detection](/docs/attack-detection) - Detection metrics
- [Policy Engine](/docs/policy-engine) - Policy enforcement metrics
- [API Reference](/docs/api-reference) - Metrics API endpoints
- [Deployment](/docs/deployment-kubernetes) - Production deployment

---

## Support

Need monitoring help?

- **Discord**: [discord.gg/koreshield](https://discord.gg/koreshield)
- **GitHub**: [github.com/koreshield/koreshield/issues](https://github.com/koreshield/koreshield/issues)
- **Email**: support@koreshield.com
- **Documentation**: [docs.koreshield.com](https://docs.koreshield.com)
